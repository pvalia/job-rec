{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 50, 20\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import time\n",
    "start=time.time()\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean skills data and make a vocabulary for skills vectorization\n",
    "common_placeholders = [ 'n/a', 'not applicable', 'none']\n",
    "\n",
    "def text_scrubber(values):\n",
    "    result = []\n",
    "    for item in values:\n",
    "        # If 'item' is a list or an array, handle it appropriately\n",
    "        if isinstance(item, list) or isinstance(item, np.ndarray):\n",
    "            item = ', '.join([str(i) for i in item])  # Convert list/array to string\n",
    "        # Now 'item' should be a string, handle NaN values and strings\n",
    "        if pd.isna(item):\n",
    "            result.append('')\n",
    "        else:\n",
    "            # Your existing cleaning code\n",
    "            temp = item.lower()  # Convert to lowercase\n",
    "            temp = re.sub(r'\\(.*\\)|&#39;|\\x92', '', temp)  # Remove unwanted characters\n",
    "            temp = re.sub(r' &amp; |&amp;|\\x95|:|;|&|\\.|/| and ', ',', temp)  # Replace certain characters with comma\n",
    "            temp = re.sub(r'\\s+', ' ', temp).strip()  # Normalize white spaces\n",
    "            # Optionally, split the skills into a list and remove empty entries\n",
    "            temp = [skill.strip() for skill in temp.split(',') if skill.strip()]\n",
    "            # Rejoin the cleaned skills into a string separated by commas\n",
    "            temp = ','.join(temp)\n",
    "            result.append(temp)\n",
    "    return result\n",
    "\n",
    "def tokenizer(df):\n",
    "    # Custom stop words that come up very often but don't say much about the job title.\n",
    "    stops = ['manager', 'nice' 'responsibilities', 'used', 'skills', 'duties', 'work', 'worked', 'daily', 'next','magic','world','interview',\n",
    "             'services', 'job', 'good','using', '.com', 'end', 'prepare', 'prepared', 'lead', 'requirements','#39','see below','yes','null'] + list(stopwords.words('english'))\n",
    "    values, ids, resume_ids = [],[],[]\n",
    "    count = 0\n",
    "    for idx, row in df.iterrows():        \n",
    "        # Split on commas\n",
    "        array = row['skills']\n",
    "        array=str(array)\n",
    "        array=array.split(',')\n",
    "        for x in array:\n",
    "            # make sure the value is not empty or all numeric values or in the stop words list\n",
    "            if x != '' and not x.lstrip().rstrip() in stops and not x.lstrip().rstrip().isdigit():\n",
    "                # make sure single character results are the letter 'C' (programming language)\n",
    "                if len(x) > 1 or x == 'C':\n",
    "                    # drop stuff > 4 gram\n",
    "                    if len(x.split(' ')) <= 4:\n",
    "                        # update lists\n",
    "                        values.append(x.lstrip().rstrip())\n",
    "                        ids.append(count)\n",
    "                        count+=1\n",
    "    \n",
    "    # New dataframe with updated values.\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df['skills'] = values\n",
    "    print(result_df)\n",
    "    return result_df\n",
    "\n",
    "#df = pd.read_csv(\"../../data/dice-shortened.csv\")\n",
    "#df = pd.read_csv(\"../karim/jobs_to_upload.csv\")\n",
    "df = pd.read_json(\"../karim/jobs.json\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skills'] = text_scrubber(df['skills'])\n",
    "print(df['skills'])\n",
    "test_df = tokenizer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = test_df['skills'].unique()\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the text in each job description\n",
    "def clean_text(text):\n",
    "    cleaned_text = text.replace(\"&nbsp;\", \" \").replace(\"\\x92\", \" \").replace(\"\\x95\", \" \").replace('&amp;', \" \") \\\n",
    "        .replace('*', \" \").replace(\".\", \" \").replace(\"co&#39;s\", \"\").replace(\"\\xae&quot;\", \"\") \\\n",
    "        .replace(\"&#39;s\", \"\").replace(\"&quot;\", \"\").replace(\"?\", \"\").replace(\"&#39;s\", \"\") \\\n",
    "        .replace(\"@\", \"\").replace(\"\\x96\", \"\")\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to each element in the 'jobdescription' column\n",
    "df['desc'] = df['desc'].apply(clean_text)\n",
    "print(df['desc'])\n",
    "df['desc'].to_csv(\"jobdesc_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_df ignores terms that are in more than 20% of documents\n",
    "mine = ['manager', 'amp', 'nbsp', 'responsibilities', 'used', 'skills', 'duties', 'work', 'worked', 'daily','services', 'job', 'using', 'com', 'end', 'prepare', 'prepared', 'lead', 'requirements','summary','Job Role','Position']\n",
    "vec = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), token_pattern='[a-zA-z]{3,50}', max_df=0.2, min_df=2, max_features=10000, stop_words=list(text.ENGLISH_STOP_WORDS.union(list(mine))), decode_error='ignore', vocabulary=None, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc_new']=df['experience']+df['title']\n",
    "df.to_csv(\"desc_new.csv\", index=False)\n",
    "description_matrix2 = vec.fit_transform(df['desc_new'].values.astype('U'))\n",
    "description_matrix2 = pd.DataFrame(description_matrix2.todense())\n",
    "description_matrix2.columns = vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses only vocab set made above for vectorization\n",
    "vec2 = TfidfVectorizer(vocabulary=voc, decode_error='ignore')\n",
    "df['skills_new']=df['skills']\n",
    "skills_matrix2 = vec2.fit_transform(df['skills_new'].values.astype('U'))\n",
    "skills_matrix2 = pd.DataFrame(skills_matrix2.todense())\n",
    "skills_matrix2.columns = vec2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitle_matrix = pd.concat([skills_matrix2, description_matrix2], axis=1)\n",
    "jobtitle_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA to reduce number of features\n",
    "pca = PCA(n_components=600, random_state=42)\n",
    "comps = pca.fit_transform(jobtitle_matrix)\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the components into a dataframe\n",
    "comps = pd.DataFrame(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltr = AgglomerativeClustering(n_clusters=8)\n",
    "cltr.fit(comps)\n",
    "df['cluster_no'] = cltr.labels_\n",
    "X = comps\n",
    "y = df['cluster_no']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "lr = LogisticRegression(C=10, penalty='l2', multi_class='multinomial', solver='sag', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "score3=lr.score(X_test, y_test)\n",
    "print(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at clusters\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE()\n",
    "g = pd.DataFrame(tsne.fit_transform(comps), columns=['one', 'two'])\n",
    "\n",
    "\n",
    "g['cluster_no'] = cltr.labels_\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Clusters Using T-SNE Components', fontsize=20)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.scatter(g['one'], g['two'], c=g['cluster_no'], cmap=cm.jet, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster number to each job title in comps to pull particular cluster out for comparison\n",
    "comps['cluster_no'] = y.values\n",
    "comps.set_index('cluster_no', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_suggestions(resume_text):\n",
    "    matches=dict()\n",
    "    # Vectorize user's skills and job descriptions\n",
    "    desc = pd.DataFrame(vec.transform([resume_text]).todense())\n",
    "    desc.columns = vec.get_feature_names_out()\n",
    "    skillz = pd.DataFrame(vec2.transform([resume_text]).todense())\n",
    "    skillz.columns = vec2.get_feature_names_out()\n",
    "    mat = pd.concat([skillz, desc], axis=1)\n",
    "    # Tranform feature matrix with pca\n",
    "    user_comps = pd.DataFrame(pca.transform(mat))\n",
    "\n",
    "    # Predict cluster for user and print cluster number\n",
    "    cluster = lr.predict(user_comps)[0]\n",
    "    print ('CLUSTER NUMBER', cluster, '\\n\\n')\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = pd.DataFrame(cosine_similarity(user_comps,comps[comps.index==cluster]))\n",
    "\n",
    "    # Get job titles from df to associate cosine similarity scores with jobs\n",
    "    samp_for_cluster = df[df['cluster_no']==cluster]\n",
    "    cos_sim = cos_sim.T.set_index(samp_for_cluster['title'])\n",
    "    cos_sim.columns = ['score']\n",
    "    \n",
    "    # Print the top ten suggested jobs for the user's cluster\n",
    "    top_cos_sim = cos_sim.sort_values('score', ascending=False)[:10]\n",
    "    print ('Top ten suggested for your cluster', '\\n', top_cos_sim, '\\n\\n')\n",
    "    \n",
    "   # print('Accuracy',)\n",
    "\n",
    "    # Print the top five suggested jobs for each cluster\n",
    "    mat = mat.T\n",
    "    for i in range(8):\n",
    "        cos_sim = pd.DataFrame(cosine_similarity(user_comps,comps[comps.index==i]))\n",
    "        samp_for_cluster = df[df['cluster_no']==i]\n",
    "        cos_sim = cos_sim.T.set_index(samp_for_cluster.index)\n",
    "        cos_sim.columns = ['score']\n",
    "        top_5 = cos_sim.sort_values('score', ascending=False)[:5]\n",
    "\n",
    "        # Merge top_5 with sample2 to get skills and description\n",
    "        merged_top_5 = top_5.merge(df, how='left', left_index=True, right_index=True)\n",
    "        print ('---------Top five suggested in cluster', i,  '---------\\n', top_5, '\\n\\n')\n",
    "        # Vectorize to find skills needed for each job title\n",
    "       \n",
    "        for job in merged_top_5.index:\n",
    "            job_skills = pd.DataFrame(vec2.transform([merged_top_5.loc[job]['desc'] + merged_top_5.loc[job]['skills']]).todense())\n",
    "            job_skills.columns = vec2.get_feature_names_out()\n",
    "            job_skills = job_skills.T\n",
    "            job_skills.columns = ['score']\n",
    "            job_skills = job_skills[job_skills['score'] != 0].sort_values('score', ascending=False)\n",
    "            mat.columns = ['score']\n",
    "            mat = mat[mat['score'] != 0]\n",
    "            needed_skills = []\n",
    "            scorey = []\n",
    "            for i in job_skills.index:\n",
    "                if i not in mat.index:    \n",
    "                    needed_skills.append(i)\n",
    "                    scorey.append(job_skills.loc[i][0])\n",
    "            top_skills = pd.DataFrame(list(zip(needed_skills, scorey)), columns=['Skills', 'Importance'])\n",
    "            print('To become a/an', job,',', '\\n', 'these are the top ten skills you need:', '\\n')\n",
    "            print(top_skills[:5], '\\n')\n",
    "    return top_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_text = ''' typescript, s, css, express, r, node.js, javascript, html, react '''\n",
    "#resume_text = '''aws, s, .net, angular, sql, css, azure, javascript, c, agile, html '''\n",
    "resume_text = '''sql,oracle,css,java,self,spring,agile,s,html,javascript,sas '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_result=give_suggestions(resume_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
